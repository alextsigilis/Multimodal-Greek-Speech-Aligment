{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70bc522e",
   "metadata": {},
   "source": [
    "# Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· ÎœÎ¿Î½Ï„Î­Î»Î¿Ï… Î•Ï…Î¸Ï…Î³ÏÎ¬Î¼Î¼Î¹ÏƒÎ·Ï‚ ÎŸÎ¼Î¹Î»Î¯Î±Ï‚â€“ÎšÎµÎ¹Î¼Î­Î½Î¿Ï…\n",
    "\n",
    "Î£Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ notebook ÎµÎºÏ€Î±Î¹Î´ÎµÏÎ¿Ï…Î¼Îµ Î­Î½Î± **contrastive alignment model** Ï€Î¿Ï… Î¼Î±Î¸Î±Î¯Î½ÎµÎ¹ Î½Î± Î±Î½Ï„Î¹ÏƒÏ„Î¿Î¹Ï‡Î¯Î¶ÎµÎ¹ Î±Î½Î±Ï€Î±ÏÎ±ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚ Î¿Î¼Î¹Î»Î¯Î±Ï‚ (speech embeddings) Î¼Îµ Î±Î½Î±Ï€Î±ÏÎ±ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… (text embeddings) ÏƒÏ„Î¿Î½ Î¯Î´Î¹Î¿ Ï‡ÏÏÎ¿.\n",
    "\n",
    "**Î‘ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ®:**\n",
    "- **Speech encoder**: Frozen Whisper-tiny â€” Ï€ÏÎ¿-Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼Î­Î½Î± hidden states `[B, T, 384]`\n",
    "- **Text encoder**: Frozen multilingual-e5-small â€” Ï€ÏÎ¿-Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼Î­Î½Î± embeddings `[B, 384]`\n",
    "- **Aligner**: Î•ÎºÏ€Î±Î¹Î´ÎµÏÏƒÎ¹Î¼Î¿Ï‚ CNN Î¼Îµ multi-kernel convolutions Ï€Î¿Ï… Î¼ÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ Ï„Î± speech hidden states ÏƒÎµ Î­Î½Î± ÎºÎ¿Î¹Î½ÏŒ embedding space\n",
    "- **Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· ÎºÏŒÏƒÏ„Î¿Ï…Ï‚**: Symmetric InfoNCE (contrastive loss) Î¼Îµ ÎµÎºÏ€Î±Î¹Î´ÎµÏÏƒÎ¹Î¼Î· Î¸ÎµÏÎ¼Î¿ÎºÏÎ±ÏƒÎ¯Î± $\\tau$\n",
    "\n",
    "**Î¡Î¿Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½:**\n",
    "$$\\text{speech\\_hidden} \\xrightarrow{\\text{CNN Aligner}} \\text{aligned} \\xrightarrow{\\text{mean pool}} \\text{speech\\_emb} \\xrightarrow{\\ell_2\\text{-norm}} \\hat{s}$$\n",
    "$$\\text{logits} = \\frac{\\hat{s} \\cdot \\hat{t}^\\top}{\\tau}, \\quad \\mathcal{L} = \\frac{1}{2}\\left[\\text{CE}(\\text{logits}, y) + \\text{CE}(\\text{logits}^\\top, y)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382eaaba",
   "metadata": {},
   "source": [
    "## 1. Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® Î’Î¹Î²Î»Î¹Î¿Î¸Î·ÎºÏÎ½\n",
    "\n",
    "Î•Î¹ÏƒÎ¬Î³Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„ÎµÏ‚ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚:\n",
    "- **PyTorch** (`torch`, `torch.nn`, `torch.nn.functional`): Î“Î¹Î± Ï„Î¿Î½ Î¿ÏÎ¹ÏƒÎ¼ÏŒ ÎºÎ±Î¹ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Î½ÎµÏ…ÏÏ‰Î½Î¹ÎºÏÎ½ Î´Î¹ÎºÏ„ÏÏ‰Î½\n",
    "- **HuggingFace Transformers**: Î“Î¹Î± Ï€ÏÏŒÏƒÎ²Î±ÏƒÎ· ÏƒÏ„Î± Ï€ÏÎ¿-ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Î¼Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î±\n",
    "- **Î•ÏÎ³Î±Î»ÎµÎ¯Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…**: `re`, `string`, `unicodedata` Î³Î¹Î± ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒ Î¼ÎµÏ„Î±Î³ÏÎ±Ï†ÏÎ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc43044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from models import mean_pooling, AlignmentModel, MlpAdapter, CnnAdapter, LstmAdapter\n",
    "from preprocess import run as precompute\n",
    "import evaluate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dae2d0",
   "metadata": {},
   "source": [
    "## 2. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½\n",
    "\n",
    "Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿ dataset **ddamianos/hparl** (Î•Î»Î»Î·Î½Î¹ÎºÎ¬ ÎšÎ¿Î¹Î½Î¿Î²Î¿Ï…Î»ÎµÏ…Ï„Î¹ÎºÎ¬ Î ÏÎ±ÎºÏ„Î¹ÎºÎ¬) Î¼Îµ Ï€ÏÎ¿-Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼Î­Î½Î± embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3b08f",
   "metadata": {},
   "source": [
    "### 2.1 Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± ÎšÎµÎ¹Î¼Î­Î½Î¿Ï…\n",
    "\n",
    "ÎŸÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `preprocess_sentence` Î³Î¹Î± Ï„Î¿Î½ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒ Ï„Ï‰Î½ Î¼ÎµÏ„Î±Î³ÏÎ±Ï†ÏÎ½:\n",
    "1. Î‘Ï†Î±Î¯ÏÎµÏƒÎ· tokens `[UNK]`\n",
    "2. Î‘Ï†Î±Î¯ÏÎµÏƒÎ· ÏƒÎ·Î¼ÎµÎ¯Ï‰Î½ ÏƒÏ„Î¯Î¾Î·Ï‚\n",
    "3. ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Unicode (NFKD)\n",
    "4. ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ Ï€ÎµÎ¶Î¬\n",
    "5. Î‘Ï†Î±Î¯ÏÎµÏƒÎ· Ï€Î»ÎµÎ¿Î½Î¬Î¶Î¿Î½Ï„Ï‰Î½ ÎºÎµÎ½ÏÎ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1e86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for the dataset\n",
    "base = \"/mnt/h/\"\n",
    "\n",
    "# Path to precomputed dataset\n",
    "PRECOMPUTED_PATH = f\"{base}/hparl-preprocessed\"\n",
    "\n",
    "# HuggingFace caches\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = f\"{base}/datasets\"\n",
    "os.environ[\"HF_HOME\"] = base\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = f\"{base}/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1164268",
   "metadata": {},
   "source": [
    "### 2.2 Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î ÏÎ¿-Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼Î­Î½Ï‰Î½ Embeddings\n",
    "\n",
    "Î¦Î¿ÏÏ„ÏÎ½Î¿Ï…Î¼Îµ Ï„o dataset Ï€Î¿Ï… Ï€Î±ÏÎ®Ï‡Î¸Î· Î±Ï€ÏŒ Ï„Î¿ `preprocess.py`. Î‘Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÏƒÏ„Î¿Î½ Î´Î¯ÏƒÎºÎ¿, ÎµÎºÏ„ÎµÎ»Î¿ÏÎ¼Îµ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Ï„Î·Î½ Ï€ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î¼Î­ÏƒÏ‰ `preprocess.run()`.\n",
    "\n",
    "Î ÎµÏÎ¹Î­Ï‡ÎµÎ¹:\n",
    "\n",
    "| Î£Ï„Î®Î»Î· | Î£Ï‡Î®Î¼Î± | Î ÎµÏÎ¹Î³ÏÎ±Ï†Î® |\n",
    "|-------|-------|-----------|\n",
    "| `pooled_speech_embeddings` | `[T, 384]` | Î§ÏÎ¿Î½Î¹ÎºÎ¬ pooled hidden states Ï„Î¿Ï… Whisper encoder |\n",
    "| `pooled_attn_masks` | `[T]` | ÎœÎ¬ÏƒÎºÎ± Ï€ÏÎ¿ÏƒÎ¿Ï‡Î®Ï‚ (1 = Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ frame, 0 = padding) |\n",
    "| `transcript_embeddings` | `[384]` | L2-ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¼Î­Î½Î± E5 embeddings ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… |\n",
    "\n",
    "ÎœÎµÏ„Î±Ï„ÏÎ­Ï€Î¿Ï…Î¼Îµ ÏƒÎµ format PyTorch tensors Î¼Îµ `.with_format('torch')`.\n",
    "\n",
    "Î¤Î¿ `test` split ÎºÏÎ±Ï„Î¹Î­Ï„Î±Î¹ **Î±Ï€Î¿ÎºÎ»ÎµÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î³Î¹Î± Ï„ÎµÎ»Î¹ÎºÎ® Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·** (evaluation). ÎšÎ±Ï„Î¬ Ï„Î·Î½ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·, Ï‡Ï‰ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ `train` split ÏƒÎµ **80% train / 20% validation** Î³Î¹Î± Ï€Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· Ï„Î¿Ï… validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e56a1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed dataset from /mnt/h//hparl-preprocessed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0deb6fd71abb4aa795ce07e1f05be724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 61072, Val: 15269, Test (held out): 8679\n",
      "Columns: ['pooled_speech_embeddings', 'pooled_attn_masks', 'transcript_embeddings']\n",
      "Speech shape: torch.Size([374, 384])\n",
      "Mask shape:   torch.Size([374])\n",
      "Text shape:   torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "# Load precomputed dataset (whisper hidden states + e5 text embeddings)\n",
    "# If it doesn't exist on disk, run preprocessing first.\n",
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "\n",
    "if Path(PRECOMPUTED_PATH).exists():\n",
    "    print(f\"Loading precomputed dataset from {PRECOMPUTED_PATH}\")\n",
    "    ds = load_from_disk(PRECOMPUTED_PATH).with_format('torch')\n",
    "else:\n",
    "    print(f\"Precomputed dataset not found at {PRECOMPUTED_PATH}, running preprocessing...\")\n",
    "    ds = precompute(base='/mnt/h', save_dir='hparl_precomputed').with_format('torch')\n",
    "    print(\"Preprocessing complete.\")\n",
    "\n",
    "# Reserve test split for final evaluation only\n",
    "test_ds = ds['test']\n",
    "\n",
    "# Split train into 80% train / 20% validation\n",
    "split = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_ds = split['train']\n",
    "val_ds = split['test']\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test (held out): {len(test_ds)}\")\n",
    "print(f\"Columns: {train_ds.column_names}\")\n",
    "print(f\"Speech shape: {train_ds[0]['pooled_speech_embeddings'].shape}\")\n",
    "print(f\"Mask shape:   {train_ds[0]['pooled_attn_masks'].shape}\")\n",
    "print(f\"Text shape:   {train_ds[0]['transcript_embeddings'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6167d1",
   "metadata": {},
   "source": [
    "## 3. ÎœÎ¿Î½Ï„Î­Î»Î± (Î±Ï€ÏŒ `models.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d6f29",
   "metadata": {},
   "source": [
    "ÎŸÎ¹ Î¿ÏÎ¹ÏƒÎ¼Î¿Î¯ Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î²ÏÎ¯ÏƒÎºÎ¿Î½Ï„Î±Î¹ ÏƒÏ„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ `models.py` ÎºÎ±Î¹ ÎµÎ¹ÏƒÎ¬Î³Î¿Î½Ï„Î±Î¹ ÏƒÏ„Î¿ cell 1.\n",
    "\n",
    "Î¤Î± Î²Î±ÏƒÎ¹ÎºÎ¬ components:\n",
    "1. **`mean_pooling`**: Mask-aware Î¼Î­ÏƒÎ¿Ï‚ ÏŒÏÎ¿Ï‚ Ï€Î¬Î½Ï‰ ÏƒÏ„Î· Ï‡ÏÎ¿Î½Î¹ÎºÎ® Î´Î¹Î¬ÏƒÏ„Î±ÏƒÎ·\n",
    "2. **`CnnAdapter`**: CNN adapter Î¼Îµ multi-kernel convolutions â€” Î¼ÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ speech hidden states `[B, T, 384]` â†’ `[B, T', 384]`\n",
    "3. **`AlignmentModel`**: Î”Î­Ï‡ÎµÏ„Î±Î¹ Î­Î½Î± adapter Ï‰Ï‚ argument, Ï€ÏÎ¿ÏƒÎ¸Î­Ï„ÎµÎ¹ mean pooling + L2 norm + symmetric InfoNCE loss\n",
    "\n",
    "**Î‘ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ® `CnnAdapter`:**\n",
    "```\n",
    "[B, T, 384] â†’ transpose â†’ [B, 384, T]\n",
    "    â†’ Conv1d(k=3) âŠ• Conv1d(k=5) âŠ• Conv1d(k=7) â†’ [B, 768, T]\n",
    "    â†’ MaxPool1d â†’ [B, 768, T/2]\n",
    "    â†’ LayerNorm â†’ (repeat) â†’ [B, 768, T/4]\n",
    "    â†’ Conv1d(1Ã—1) â†’ [B, 384, T/4]\n",
    "    â†’ transpose â†’ [B, T/4, 384]\n",
    "```\n",
    "\n",
    "**Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· ÎºÏŒÏƒÏ„Î¿Ï…Ï‚ InfoNCE:**\n",
    "\n",
    "$$\\mathcal{L} = \\gamma\\;\\text{CE}\\left(\\frac{S \\cdot T^\\top}{\\tau},\\, y\\right) + (1-\\gamma)\\text{CE}\\left(\\frac{T \\cdot S^\\top}{\\tau},\\, y\\right)$$\n",
    "\n",
    "ÏŒÏ€Î¿Ï… $S, T \\in \\mathbb{R}^{B \\times D}$ Ï„Î± ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¼Î­Î½Î± embeddings, $\\tau = e^{\\log\\tau}$ Î· ÎµÎºÏ€Î±Î¹Î´ÎµÏÏƒÎ¹Î¼Î· Î¸ÎµÏÎ¼Î¿ÎºÏÎ±ÏƒÎ¯Î±."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c689653",
   "metadata": {},
   "source": [
    "## 4. Pipeline Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚ â€” PyTorch Lightning\n",
    "\n",
    "Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ **PyTorch Lightning** Î³Î¹Î± Ï„Î· Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Ï„Î¿Ï… training loop.\n",
    "\n",
    "### 4.1 `PrecomputedDataset` & `collate_fn`\n",
    "\n",
    "- **`PrecomputedDataset`**: Î•Î»Î±Ï†ÏÏ `torch.Dataset` wrapper Î³ÏÏÏ‰ Î±Ï€ÏŒ Î­Î½Î± HuggingFace split. ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ ÎºÎ¬Î¸Îµ Î´ÎµÎ¯Î³Î¼Î± ÏƒÎµ `dict` Î¼Îµ tensors `speech`, `mask`, `text`.\n",
    "- **`collate_fn`**: Custom collator Ï€Î¿Ï… ÎºÎ¬Î½ÎµÎ¹ **padding ÏƒÏ„Î· Ï‡ÏÎ¿Î½Î¹ÎºÎ® Î´Î¹Î¬ÏƒÏ„Î±ÏƒÎ·** (Ï„Î± speech embeddings Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î­Ï‡Î¿Ï…Î½ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏŒ Î¼Î®ÎºÎ¿Ï‚ Î»ÏŒÎ³Ï‰ Ï„Î·Ï‚ Î¼Î¬ÏƒÎºÎ±Ï‚) ÎºÎ±Î¹ stack Ï„Î± text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0cef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecomputedDataset(Dataset):\n",
    "    \"\"\"Thin wrapper around an HF split with precomputed embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, hf_split):\n",
    "        self.ds = hf_split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        return {\n",
    "            'speech': item['pooled_speech_embeddings'].detach().clone() if isinstance(item['pooled_speech_embeddings'], torch.Tensor) else torch.tensor(item['pooled_speech_embeddings'], dtype=torch.float32),\n",
    "            'mask':   item['pooled_attn_masks'].detach().clone() if isinstance(item['pooled_attn_masks'], torch.Tensor) else torch.tensor(item['pooled_attn_masks'], dtype=torch.long),\n",
    "            'text':   item['transcript_embeddings'].detach().clone() if isinstance(item['transcript_embeddings'], torch.Tensor) else torch.tensor(item['transcript_embeddings'], dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad speech/mask in time dimension, stack text embeddings.\"\"\"\n",
    "    speech_list = [b['speech'] for b in batch]\n",
    "    mask_list   = [b['mask'] for b in batch]\n",
    "    text_list   = [b['text'] for b in batch]\n",
    "\n",
    "    max_T = max(s.shape[0] for s in speech_list)\n",
    "    D = speech_list[0].shape[1]\n",
    "\n",
    "    padded_speech = torch.zeros(len(batch), max_T, D)\n",
    "    padded_mask   = torch.zeros(len(batch), max_T, dtype=torch.long)\n",
    "\n",
    "    for i, (s, m) in enumerate(zip(speech_list, mask_list)):\n",
    "        T = s.shape[0]\n",
    "        padded_speech[i, :T] = s\n",
    "        padded_mask[i, :T] = m\n",
    "\n",
    "    return {\n",
    "        'speech': padded_speech,\n",
    "        'mask':   padded_mask,\n",
    "        'text':   torch.stack(text_list),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa8248",
   "metadata": {},
   "source": [
    "### 4.2 `AlignmentLitModule`\n",
    "\n",
    "Lightning wrapper Î³ÏÏÏ‰ Î±Ï€ÏŒ Ï„Î¿ `AlignmentModel`. Î”Î­Ï‡ÎµÏ„Î±Î¹ Î­Î½Î± **adapter** (Ï€.Ï‡. `CnnAdapter`) Ï‰Ï‚ argument:\n",
    "- **`training_step`** / **`validation_step`**: Forward pass + loss logging\n",
    "- **`configure_optimizers`**: AdamW optimizer Î¼ÏŒÎ½Î¿ Î³Î¹Î± Ï„Î¹Ï‚ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï…Ï‚ Ï„Î¿Ï… adapter ÎºÎ±Î¹ Ï„Î· Î¸ÎµÏÎ¼Î¿ÎºÏÎ±ÏƒÎ¯Î± $\\log\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9046f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignmentLitModule(L.LightningModule):\n",
    "    \"\"\"Lightning wrapper around AlignmentModel (precomputed embeddings).\"\"\"\n",
    "\n",
    "    def __init__(self, adapter: nn.Module, lr=1e-4, weight_decay=0.0, init_tau=0.07):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['adapter'])\n",
    "        self.model = AlignmentModel(adapter=adapter, init_tau=init_tau)\n",
    "\n",
    "    def forward(self, speech, mask, text):\n",
    "        return self.model(speech, mask, text)\n",
    "\n",
    "    def _shared_step(self, batch, stage):\n",
    "        out = self.model(batch['speech'], batch['mask'], batch['text'])\n",
    "        # Log the learnable temperature (log_tau) and tau=exp(log_tau) to TensorBoard\n",
    "        # Logged at epoch granularity to avoid excessive scalar logging.\n",
    "        self.log('log_tau', self.model.log_tau,\n",
    "                 prog_bar=False, on_epoch=True, on_step=False)\n",
    "        self.log('tau', torch.exp(self.model.log_tau),\n",
    "                 prog_bar=False, on_epoch=True, on_step=False)\n",
    "\n",
    "        if stage == 'val':\n",
    "            self.log('val_loss',\n",
    "                     out['loss'],\n",
    "                     prog_bar=True,\n",
    "                     batch_size=batch['speech'].size(0),\n",
    "                     on_epoch=True,\n",
    "                     on_step=False)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out = self._shared_step(batch, 'train')\n",
    "        self.log('train_loss',\n",
    "                 out['loss'],\n",
    "                 prog_bar=True,\n",
    "                 batch_size=batch['speech'].size(0), \n",
    "                 on_epoch=True, \n",
    "                 on_step=False)\n",
    "        return out['loss']\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._shared_step(batch, 'val')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.model.adapter.parameters()) + [self.model.log_tau]\n",
    "        return torch.optim.AdamW(params, lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa9ce0",
   "metadata": {},
   "source": [
    "### 4.3 `SpeechTextDataModule`\n",
    "\n",
    "Lightning `DataModule` Ï€Î¿Ï… ÎµÎ½Î¸Ï…Î»Î±ÎºÏÎ½ÎµÎ¹ Ï„Î± train/val dataloaders. Î Î±ÏÎ±Î¼ÎµÏ„ÏÎ¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Î¼Îµ:\n",
    "- `batch_size`: ÎœÎ­Î³ÎµÎ¸Î¿Ï‚ batch (default: 64)\n",
    "- `num_workers`: Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ worker processes Î³Î¹Î± Ï€Î±ÏÎ¬Î»Î»Î·Î»Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "240b7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechTextDataModule(L.LightningDataModule):\n",
    "    \"\"\"DataModule for precomputed speech/text embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, train_ds, val_ds, batch_size=64, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.train_dataset = PrecomputedDataset(train_ds)\n",
    "        self.val_dataset = PrecomputedDataset(val_ds)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6e18c",
   "metadata": {},
   "source": [
    "## 5. Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·\n",
    "\n",
    "Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚:\n",
    "- **Epochs**: 10\n",
    "- **Batch size**: 64\n",
    "- **Learning rate**: $10^{-4}$ (AdamW)\n",
    "- **Precision**: Mixed precision (FP16) Î³Î¹Î± Ï„Î±Ï‡ÏÏ„ÎµÏÎ· ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·\n",
    "- **Validation**: ÎšÎ¬Î¸Îµ 25% Ï„Î¿Ï… training epoch (`val_check_interval=0.25`)\n",
    "- **Accelerator**: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Î±Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· GPU/CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5260f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_ds, val_ds, adapter,\n",
    "                 batch_size=64, \n",
    "                 lr=1e-4, \n",
    "                 max_epochs=20, \n",
    "                 input_checkpoint=None,\n",
    "                 checkpoint_dir=\"./checkpoint\", \n",
    "                 output_checkpoint_name=\"cnn_alignment_model.ckpt\", \n",
    "                 logger_name=\"alignment\", \n",
    "                 log_dir=\"./checkpoint/logs\", \n",
    "                 accumulate_grad_batches=8, \n",
    "                 num_workers=4):\n",
    "    \"\"\"\n",
    "    Train or resume a speech-text alignment model with configurable parameters.\n",
    "\n",
    "    Args:\n",
    "        train_ds: Training dataset (HuggingFace split).\n",
    "        val_ds: Validation dataset (HuggingFace split).\n",
    "        batch_size (int): Batch size for training.\n",
    "        lr (float): Learning rate for optimizer.\n",
    "        max_epochs (int): Number of epochs to train.\n",
    "        input_checkpoint (str, optional): Path to input checkpoint to load weights from.\n",
    "        checkpoint_dir (str): Directory to save the output checkpoint.\n",
    "        output_checkpoint_name (str): Name of the output checkpoint file.\n",
    "        logger_name (str): Name for TensorBoard logger.\n",
    "        log_dir (str, optional): Directory for TensorBoard logs. Defaults to output_checkpoint_dir/logs.\n",
    "        accumulate_grad_batches (int): Number of batches for gradient accumulation.\n",
    "        num_workers (int): Number of workers for DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        lit_model: The trained LightningModule.\n",
    "        trainer: The PyTorch Lightning Trainer instance.\n",
    "    \"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    if log_dir is None:\n",
    "        log_dir = os.path.join(checkpoint_dir, \"logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Set up TensorBoard logger\n",
    "    logger = TensorBoardLogger(log_dir, name=logger_name)\n",
    "\n",
    "    input_checkpoint_path = os.path.join(checkpoint_dir, input_checkpoint) if input_checkpoint else None\n",
    "    output_ckpt_path = os.path.join(checkpoint_dir, output_checkpoint_name)\n",
    "\n",
    "    if input_checkpoint_path is not None and os.path.exists(input_checkpoint_path):\n",
    "        print(f\"Loading model from input checkpoint: {input_checkpoint_path}\")\n",
    "        lit_model = AlignmentLitModule.load_from_checkpoint(input_checkpoint_path, adapter=adapter, lr=lr)\n",
    "    else:\n",
    "        print(\"No input checkpoint found or provided. Training from scratch.\")\n",
    "        lit_model = AlignmentLitModule(adapter=adapter, lr=lr)\n",
    "\n",
    "    dm = SpeechTextDataModule(\n",
    "        train_ds=train_ds,\n",
    "        val_ds=val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='auto',\n",
    "        precision='16-mixed',\n",
    "        log_every_n_steps=10,\n",
    "        val_check_interval=0.25,\n",
    "        logger=logger,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "    )\n",
    "\n",
    "    trainer.fit(lit_model,\n",
    "                datamodule=dm,\n",
    "                ckpt_path=input_checkpoint_path if input_checkpoint_path and os.path.exists(input_checkpoint_path) else None)\n",
    "\n",
    "    trainer.save_checkpoint(output_ckpt_path)\n",
    "    print(f\"Model checkpoint saved to {output_ckpt_path}\")\n",
    "    print(f\"TensorBoard logs saved to {log_dir}/{logger_name}\")\n",
    "    return lit_model, trainer\n",
    "\n",
    "# Example usage:\n",
    "# lit_model, trainer = run_training(train_ds, val_ds, adapter, batch_size=64, lr=1e-4, max_epochs=10, input_checkpoint=None, checkpoint_dir=\"/mnt/h/outputs/checkpoints/my_ckpt\", output_checkpoint_name=\"my_model.ckpt\", logger_name=\"my_log\", log_dir=\"/mnt/h/outputs/logs/my_log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11d55b",
   "metadata": {},
   "source": [
    "### 5.1 Î™ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b9e28",
   "metadata": {},
   "source": [
    "#### MLP Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = MlpAdapter(\n",
    "        speech_dim=384,\n",
    "        text_dim=384,\n",
    "        hidden_dim=256,\n",
    "        num_layers=5,\n",
    "        dropout=0.1,)\n",
    "\n",
    "checkpoint_dir = \"/mnt/h/outputs/checkpoints/mlp_aligner_0\"\n",
    "log_dir = \"./logs\"\n",
    "logger_name = \"mlp_alignment_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_checkpoint_name = None\n",
    "output_checkpoint_name = \"model_0.ckpt\"\n",
    "\n",
    "run_training(train_ds, val_ds, adapter,\n",
    "             batch_size=128,\n",
    "             lr=5e-4,\n",
    "             max_epochs=40,\n",
    "             checkpoint_dir=checkpoint_dir,\n",
    "             input_checkpoint=input_checkpoint_name,\n",
    "             output_checkpoint_name=output_checkpoint_name,\n",
    "             logger_name=logger_name,\n",
    "             log_dir=log_dir,\n",
    "             accumulate_grad_batches=4,\n",
    "             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation results\n",
    "recall_scores, mrr_score = evaluate.run(\n",
    "    ckpt_dir='/mnt/h/outputs/checkpoints/mlp_aligner_0/',\n",
    "    ckpt_name=f'model_0.ckpt',\n",
    "    data_dir=\"/mnt/h/hparl-preprocessed\",\n",
    "    k_values=[1, 3, 5, 10, 15, 20, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89af5fc8",
   "metadata": {},
   "source": [
    "#### CNN Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aacb665",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = CnnAdapter(\n",
    "        speech_dim=384,\n",
    "        text_dim=384,\n",
    "        hidden_dim=256,\n",
    "        kernel_sizes=(3, 5, 7),\n",
    "        num_layers=2,\n",
    "        pool_stride=2,\n",
    "        dropout=0.1,)\n",
    "\n",
    "checkpoint_dir = \"/mnt/h/outputs/checkpoints/cnn_aligner_0\"\n",
    "log_dir = \"./logs\"\n",
    "logger_name = \"cnn_alignment_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9bdea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_checkpoint_name = None\n",
    "output_checkpoint_name = \"model_0.ckpt\"\n",
    "\n",
    "run_training(train_ds, val_ds, adapter,\n",
    "             batch_size=128,\n",
    "             lr=5e-4,\n",
    "             max_epochs=40,\n",
    "             checkpoint_dir=checkpoint_dir,\n",
    "             input_checkpoint=input_checkpoint_name,\n",
    "             output_checkpoint_name=output_checkpoint_name,\n",
    "             logger_name=logger_name,\n",
    "             log_dir=log_dir,\n",
    "             accumulate_grad_batches=4,\n",
    "             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation results\n",
    "recall_scores, mrr_score = evaluate.run(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    ckpt_name=f'model_0.ckpt',\n",
    "    data_dir=\"/mnt/h/hparl-preprocessed\",\n",
    "    k_values=[1, 3, 5, 10, 15, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d6b4a",
   "metadata": {},
   "source": [
    "#### LSTM Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc67e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = LstmAdapter(\n",
    "        speech_dim=384,\n",
    "        text_dim=384,\n",
    "        hidden_dim=256,\n",
    "        num_layers=3,\n",
    "        dropout=0.1,)\n",
    "\n",
    "checkpoint_dir = \"/mnt/h/outputs/checkpoints/lstm_aligner_0\"\n",
    "log_dir = \"./logs\"\n",
    "logger_name = \"lstm_alignment_0\"\n",
    "\n",
    "adapter.save_config(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1bd8108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "ğŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n",
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No input checkpoint found or provided. Training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/sted/.venvs/diplomatiki/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name  </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type           </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>â”ƒ\n",
       "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>â”‚ model â”‚ AlignmentModel â”‚  1.8 M â”‚ train â”‚     0 â”‚\n",
       "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType          \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0mâ”‚ model â”‚ AlignmentModel â”‚  1.8 M â”‚ train â”‚     0 â”‚\n",
       "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.8 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.8 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 7                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 4                                                                                           \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.8 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.8 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 7                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 4                                                                                           \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e8dc284df049a1be2e5f6634e5490a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`weights_only` was not set, defaulting to `False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved to /mnt/h/outputs/checkpoints/lstm_aligner_0/model_0.ckpt\n",
      "TensorBoard logs saved to ./logs/lstm_alignment_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(AlignmentLitModule(\n",
       "   (model): AlignmentModel(\n",
       "     (adapter): LstmAdapter(\n",
       "       (lstm): LSTM(384, 256, num_layers=3, batch_first=True, dropout=0.1)\n",
       "       (proj): Linear(in_features=256, out_features=384, bias=True)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " <lightning.pytorch.trainer.trainer.Trainer at 0x7306e8a72000>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_checkpoint_name = None\n",
    "output_checkpoint_name = \"model_0.ckpt\"\n",
    "\n",
    "run_training(train_ds, val_ds, adapter,\n",
    "             batch_size=128,\n",
    "             lr=5e-4,\n",
    "             max_epochs=40,\n",
    "             checkpoint_dir=checkpoint_dir,\n",
    "             input_checkpoint=input_checkpoint_name,\n",
    "             output_checkpoint_name=output_checkpoint_name,\n",
    "             logger_name=logger_name,\n",
    "             log_dir=log_dir,\n",
    "             accumulate_grad_batches=4,\n",
    "             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1b03aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sted/.venvs/diplomatiki/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "[INFO] Using device: cuda\n",
      "[INFO] Loading dataset from /mnt/h/hparl-preprocessed ...\n",
      "Loading dataset from disk: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:00<00:00, 5731.66it/s]\n",
      "[INFO] Computing embeddings for test set...\n",
      "Computing embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:26<00:00,  2.55it/s]\n",
      "[INFO] Computing similarity matrix...\n",
      "Similarity matrix: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:00<00:00, 1472.29it/s]\n",
      "\n",
      "===== TEXT â†’ SPEECH RETRIEVAL =====\n",
      "Recall@1: 0.9310\n",
      "Recall@3: 0.9675\n",
      "Recall@5: 0.9770\n",
      "Recall@10: 0.9847\n",
      "Recall@15: 0.9881\n",
      "Recall@20: 0.9902\n",
      "MRR: 0.9512\n",
      "[OK] Saved plot â†’ /mnt/h/outputs/checkpoints/lstm_aligner_0/evaluation_results_model_0/recall_curve_text_to_speech.png\n",
      "[OK] Results saved to /mnt/h/outputs/checkpoints/lstm_aligner_0/evaluation_results_model_0\n"
     ]
    }
   ],
   "source": [
    "# Evaluation results\n",
    "!python evaluate.py --ckpt-dir /mnt/h/outputs/checkpoints/lstm_aligner_0\\\n",
    "    --ckpt-name model_0.ckpt \\\n",
    "    --data-dir /mnt/h/hparl-preprocessed \\\n",
    "    --k 1 3 5 10 15 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b099d614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplomatiki (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
