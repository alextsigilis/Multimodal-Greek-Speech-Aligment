{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70bc522e",
   "metadata": {},
   "source": [
    "# Joint Aligment Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc43044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string, unicodedata\n",
    "import torch\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer, AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dae2d0",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d46eb77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ae71c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(examples):\n",
    "    \"\"\"\n",
    "    Preprocess the sentence column (batch):\n",
    "    1. Remove [UNK] tokens\n",
    "    2. Remove punctuation\n",
    "    3. Normalize text (lowercase, whitespace, unicode normalization)\n",
    "    4. Tokenize with multilingual-e5-small tokenizer\n",
    "    \"\"\"\n",
    "    texts = examples['sentence']\n",
    "    \n",
    "    # Process all texts\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        # Remove [UNK] tokens\n",
    "        text = re.sub(r'\\[UNK\\]', '', text)\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Unicode normalization (NFKD)\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        processed_texts.append(text)\n",
    "    \n",
    "    return {'gt-transcription': processed_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e56a1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8679/8679 [00:33<00:00, 259.27 examples/s]\n",
      "Map: 100%|██████████| 76341/76341 [05:03<00:00, 251.30 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>audio</th>\n",
       "      <th>gt-transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[UNK] και συναδελφοι θα προσπαθησω να μη γινω ...</td>\n",
       "      <td>{'array': [-0.0051879883, -0.0065307617, 0.007...</td>\n",
       "      <td>και συναδελφοι θα προσπαθησω να μη γινω και εγ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK] οικονομολογος διοτι δεν ειμαι οικονομολο...</td>\n",
       "      <td>{'array': [-0.1210022, -0.08911133, -0.0612792...</td>\n",
       "      <td>οικονομολογος διοτι δεν ειμαι οικονομολογος οπ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>θα αναφερω για τον προ [UNK]</td>\n",
       "      <td>{'array': [0.10360718, 0.089660645, 0.08279419...</td>\n",
       "      <td>θα αναφερω για τον προ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[UNK] γνωστα κοινα σε ολους και κοινης</td>\n",
       "      <td>{'array': [-0.08016968, -0.0892334, -0.1252441...</td>\n",
       "      <td>γνωστα κοινα σε ολους και κοινης</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>αποδοχης η κυβερνηση λοιπον παρουσιαζει τον</td>\n",
       "      <td>{'array': [0.0038146973, -0.0016174316, -0.004...</td>\n",
       "      <td>αποδοχης η κυβερνηση λοιπον παρουσιαζει τον</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [UNK] και συναδελφοι θα προσπαθησω να μη γινω ...   \n",
       "1  [UNK] οικονομολογος διοτι δεν ειμαι οικονομολο...   \n",
       "2                      θα αναφερω για τον προ [UNK]    \n",
       "3            [UNK] γνωστα κοινα σε ολους και κοινης    \n",
       "4       αποδοχης η κυβερνηση λοιπον παρουσιαζει τον    \n",
       "\n",
       "                                               audio  \\\n",
       "0  {'array': [-0.0051879883, -0.0065307617, 0.007...   \n",
       "1  {'array': [-0.1210022, -0.08911133, -0.0612792...   \n",
       "2  {'array': [0.10360718, 0.089660645, 0.08279419...   \n",
       "3  {'array': [-0.08016968, -0.0892334, -0.1252441...   \n",
       "4  {'array': [0.0038146973, -0.0016174316, -0.004...   \n",
       "\n",
       "                                    gt-transcription  \n",
       "0  και συναδελφοι θα προσπαθησω να μη γινω και εγ...  \n",
       "1  οικονομολογος διοτι δεν ειμαι οικονομολογος οπ...  \n",
       "2                             θα αναφερω για τον προ  \n",
       "3                   γνωστα κοινα σε ολους και κοινης  \n",
       "4        αποδοχης η κυβερνηση λοιπον παρουσιαζει τον  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base directory for the dataset\n",
    "base = \"/mnt/h/\"\n",
    "\n",
    "# HuggingFace caches\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = f\"{base}/datasets\"\n",
    "os.environ[\"HF_HOME\"] = base\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = f\"{base}/models\"\n",
    "\n",
    "# Base directory for the dataset\n",
    "orig_ds = load_dataset('ddamianos/hparl',\n",
    "                       cache_dir=os.environ[\"HF_DATASETS_CACHE\"])\n",
    "\n",
    "\n",
    "# Remove unnecessary columns\n",
    "for split in orig_ds:\n",
    "    orig_ds[split] = orig_ds[split].map(\n",
    "        preprocess_sentence,\n",
    "        batched=True,\n",
    "        batch_size=256\n",
    "    ).remove_columns(\n",
    "        [col for col in orig_ds[split].column_names if col not in ['audio', 'sentence']]\n",
    "    )\n",
    "    \n",
    "# Split train and test\n",
    "test_ds = orig_ds['test']\n",
    "train_ds = orig_ds['train']\n",
    "\n",
    "# Diplay some samples\n",
    "train_ds.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6167d1",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56caf68",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2685c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(hidden_state, mask):\n",
    "    '''\n",
    "    Mean pooling with an attention mask.\n",
    "\n",
    "    Computes a mask-aware mean over the temporal dimension of a sequence of \n",
    "    hidden states. Only positions where the mask is 1 contribute to the mean.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_state : torch.Tensor\n",
    "        Input tensor of shape ``[B, T, D]`` containing the sequence of hidden\n",
    "        representations (e.g., token embeddings from a transformer).\n",
    "\n",
    "    mask : torch.Tensor\n",
    "        Attention mask of shape ``[B, T]`` with values in ``{0, 1}``.\n",
    "        Positions with value ``1`` are included in the pooling operation,\n",
    "        while positions with value ``0`` are ignored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Tensor of shape ``[B, D]`` containing the mean-pooled representations\n",
    "        for each example in the batch.\n",
    "    '''\n",
    "    if mask.dim() == 2:\n",
    "        mask = mask.unsqueeze(-1)\n",
    "    masked_hidden = hidden_state * mask\n",
    "    sum_hidden = masked_hidden.sum(dim=1)\n",
    "    lengths = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    pooled = sum_hidden / lengths\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f276f4",
   "metadata": {},
   "source": [
    "### CNN Aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43df6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnAligner(nn.Module):\n",
    "    '''\n",
    "    CNN Aligner with multi-kernel convolutions and temporal downsampling.\n",
    "\n",
    "    input:  speech_embs [B, T, speech_dim]\n",
    "    output: aligned_embs [B, T', text_dim]\n",
    "            pooled_mask  [B, T']\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 speech_dim: int = 384,\n",
    "                 text_dim: int = 384,\n",
    "                 hidden_dim: int = 256,\n",
    "                 kernel_sizes=(3, 5, 7),\n",
    "                 num_layers: int = 2,\n",
    "                 pool_stride: int = 2,\n",
    "                 dropout: float = 0.1,\n",
    "                 **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        speech_dim = int(speech_dim)\n",
    "        text_dim = int(text_dim)\n",
    "        hidden_dim = int(hidden_dim)\n",
    "        kernel_sizes = [int(k) for k in kernel_sizes]\n",
    "\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.pool_stride = pool_stride\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        in_channels = speech_dim\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            layer_convs = nn.ModuleList([\n",
    "                nn.Conv1d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=hidden_dim,\n",
    "                    kernel_size=k,\n",
    "                    padding=k // 2\n",
    "                )\n",
    "                for k in kernel_sizes\n",
    "            ])\n",
    "            self.convs.append(layer_convs)\n",
    "            self.norms.append(nn.LayerNorm(hidden_dim * len(kernel_sizes)))\n",
    "            in_channels = hidden_dim * len(kernel_sizes)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(\n",
    "            kernel_size=pool_stride,\n",
    "            stride=pool_stride\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Conv1d(in_channels, text_dim, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "        self.speech_dim = speech_dim\n",
    "        self.text_dim = text_dim\n",
    "\n",
    "    def forward(self, speech_embs, attn_mask):\n",
    "        \"\"\"\n",
    "        speech_embs: [B, T, speech_dim]\n",
    "        attn_mask:   [B, T]\n",
    "        \"\"\"\n",
    "\n",
    "        # [B, C, T]\n",
    "        x = speech_embs.transpose(1, 2)\n",
    "        mask = attn_mask\n",
    "\n",
    "        for convs, norm in zip(self.convs, self.norms):\n",
    "            # Multi-kernel conv\n",
    "            feats = [self.act(conv(x)) for conv in convs]\n",
    "            x = torch.cat(feats, dim=1)   # [B, C', T]\n",
    "            x = self.dropout(x)\n",
    "\n",
    "            # Pool in time\n",
    "            x = self.pool(x)\n",
    "            mask = self._pool_mask(mask)\n",
    "\n",
    "            # LayerNorm over channels\n",
    "            x = x.transpose(1, 2)          # [B, T', C']\n",
    "            x = norm(x)\n",
    "            x = x.transpose(1, 2)          # [B, C', T']\n",
    "\n",
    "        x = self.proj(x)                  # [B, text_dim, T']\n",
    "        x = x.transpose(1, 2)             # [B, T', text_dim]\n",
    "\n",
    "        return x, mask\n",
    "\n",
    "    def _pool_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Downsample attention mask consistently with temporal pooling.\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            return None\n",
    "\n",
    "        mask = mask.unsqueeze(1).float()  # [B, 1, T]\n",
    "        mask = F.max_pool1d(\n",
    "            mask,\n",
    "            kernel_size=self.pool_stride,\n",
    "            stride=self.pool_stride,\n",
    "        )\n",
    "        return mask.squeeze(1).long() # [B, T']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff0d89",
   "metadata": {},
   "source": [
    "### Full Aligner Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abc3cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignmentModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model for learning a joint representation between speech segments and text.\n",
    "\n",
    "    Initializes a frozen Whisper-tiny encoder (speech) and a frozen\n",
    "    multilingual-e5-small encoder (text). Neither backbone is trainable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_dim=256,\n",
    "                 kernel_sizes=(3, 5, 7), \n",
    "                 num_layers=2,\n",
    "                 pool_stride=2, \n",
    "                 dropout=0.1,\n",
    "                 init_tau=0.07):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---- Speech encoder (Whisper-tiny) ----\n",
    "        self.speech_encoder = AutoModel.from_pretrained(\"openai/whisper-tiny\")\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n",
    "        for param in self.speech_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # ---- Text encoder (multilingual-e5-small) ----\n",
    "        self.text_encoder = AutoModel.from_pretrained(\"intfloat/multilingual-e5-small\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-small\")\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # ---- Aligner ----\n",
    "        self.aligner = CnnAligner(\n",
    "            speech_dim=self.speech_encoder.config.d_model,\n",
    "            text_dim=self.text_encoder.config.hidden_size,\n",
    "            hidden_dim=hidden_dim,\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            num_layers=num_layers,\n",
    "            pool_stride=pool_stride,\n",
    "            dropout=dropout)\n",
    "\n",
    "        # ---- Learnable temperature ----\n",
    "        self.log_tau = nn.Parameter(\n",
    "            torch.log(torch.tensor(init_tau, dtype=torch.float32))\n",
    "        )\n",
    "\n",
    "    def preprocess(self, audio, text):\n",
    "        \"\"\"\n",
    "        Preprocess raw audio and text into model-ready tensors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        audio : dict\n",
    "            Batched audio dictionary with keys:\n",
    "              - 'array'         : list of 1-D waveform arrays, one per sample\n",
    "              - 'sampling_rate' : int, sampling rate shared by all samples\n",
    "        text : list[str]\n",
    "            List of input strings, one per sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        input_features : Tensor [B, n_mels, T]\n",
    "            Mel-spectrogram features for the speech encoder.\n",
    "        tok : dict[str, Tensor]\n",
    "            Tokenized text inputs (input_ids, attention_mask, etc.).\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        features = self.feature_extractor(\n",
    "            audio['array'],\n",
    "            sampling_rate=audio['sampling_rate'],\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "        )\n",
    "        input_features = features.input_features.to(device)\n",
    "\n",
    "        tok = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        tok = {k: v.to(device) for k, v in tok.items()}\n",
    "\n",
    "        return input_features, tok\n",
    "\n",
    "    def forward(self, audio, text):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        audio : dict\n",
    "            Batched audio dictionary with keys:\n",
    "              - 'array'         : list of 1-D waveform arrays, one per sample\n",
    "              - 'sampling_rate' : int, sampling rate shared by all samples\n",
    "        text : list[str]\n",
    "            List of input strings, one per sample (same length as audio['array']).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict with keys:\n",
    "          - 'loss'       : scalar, symmetric InfoNCE contrastive loss\n",
    "          - 'logits'     : Tensor [B, B], cosine-similarity logits scaled by τ\n",
    "          - 'speech_emb' : Tensor [B, D], L2-normalised speech embeddings\n",
    "          - 'text_emb'   : Tensor [B, D], L2-normalised text embeddings\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        input_features, tok = self.preprocess(audio, text)\n",
    "\n",
    "        # ---- Speech branch ----\n",
    "        speech_hidden = self.speech_encoder.encoder(\n",
    "            input_features\n",
    "        ).last_hidden_state  # [B, T', speech_dim]\n",
    "\n",
    "        speech_mask = torch.ones(\n",
    "            speech_hidden.shape[:2], dtype=torch.long, device=device\n",
    "        )\n",
    "\n",
    "        aligned_speech, aligned_mask = self.aligner(speech_hidden, speech_mask)\n",
    "        speech_emb = mean_pooling(aligned_speech, aligned_mask)  # [B, D]\n",
    "\n",
    "        # ---- Text branch ----\n",
    "        text_hidden = self.text_encoder(**tok).last_hidden_state  # [B, T, D]\n",
    "        text_emb = mean_pooling(text_hidden, tok['attention_mask'])  # [B, D]\n",
    "\n",
    "        # ---- Logits & loss ----\n",
    "        speech_emb = F.normalize(speech_emb, dim=-1)\n",
    "        text_emb = F.normalize(text_emb, dim=-1)\n",
    "\n",
    "        tau = torch.exp(self.log_tau)\n",
    "        logits = (speech_emb @ text_emb.T) / tau  # [B, B]\n",
    "\n",
    "        labels = torch.arange(logits.size(0), device=device)\n",
    "        loss = 0.5 * (\n",
    "            F.cross_entropy(logits, labels)\n",
    "            + F.cross_entropy(logits.T, labels)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "            'speech_emb': speech_emb,\n",
    "            'text_emb': text_emb,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae0cef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class AlignmentLitModule(L.LightningModule):\n",
    "    \"\"\"Lightning wrapper around AlignmentModel.\"\"\"\n",
    "\n",
    "    def __init__(self, lr=1e-4, weight_decay=0.0, **model_kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = AlignmentModel(**model_kwargs)\n",
    "\n",
    "    def forward(self, audio, text):\n",
    "        return self.model(audio, text)\n",
    "\n",
    "    def _shared_step(self, batch, stage):\n",
    "        audio = {\n",
    "            'array': [a['array'] for a in batch['audio']],\n",
    "            'sampling_rate': batch['audio'][0]['sampling_rate'],\n",
    "        }\n",
    "        text = batch['sentence']\n",
    "        out = self.model(audio, text)\n",
    "        self.log(f'{stage}_loss', out['loss'], prog_bar=True, batch_size=len(text))\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, 'train')['loss']\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._shared_step(batch, 'val')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Only train aligner + temperature (backbones are frozen)\n",
    "        params = list(self.model.aligner.parameters()) + [self.model.log_tau]\n",
    "        return torch.optim.AdamW(params, lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "240b7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechTextDataModule(L.LightningDataModule):\n",
    "    \"\"\"Lightning DataModule wrapping a HuggingFace DatasetDict with 'audio' and 'sentence' columns.\"\"\"\n",
    "\n",
    "    def __init__(self, train_ds, val_ds, batch_size=8, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    @staticmethod\n",
    "    def _collate(batch):\n",
    "        \"\"\"Keep raw dicts — AlignmentModel.preprocess handles conversion.\"\"\"\n",
    "        return {\n",
    "            'audio': [item['audio'] for item in batch],\n",
    "            'sentence': [item['sentence'] for item in batch],\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self._collate,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self._collate,\n",
    "            pin_memory=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Instantiate modules ----\n",
    "lit_model = AlignmentLitModule(lr=1e-4)\n",
    "\n",
    "dm = SpeechTextDataModule(\n",
    "    train_ds=train_ds,\n",
    "    val_ds=test_ds,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "# ---- Trainer ----\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='auto',\n",
    "    precision='16-mixed',\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=0.25,  # validate 4 times per epoch\n",
    ")\n",
    "\n",
    "trainer.fit(lit_model, datamodule=dm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplomatiki (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
